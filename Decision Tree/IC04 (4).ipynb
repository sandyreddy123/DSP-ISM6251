{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "803f797e-9f7d-45db-a702-24668762abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scipy import stats\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86672aea-129e-487c-b587-51a162da9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"C:/Users/ssand/OneDrive/Desktop/v1/airbnb_train_X_price_gte_150.csv\")\n",
    "X_test = pd.read_csv(\"C:/Users/ssand/OneDrive/Desktop/v1/airbnb_test_X_price_gte_150.csv\")\n",
    "y_train = pd.read_csv(\"C:/Users/ssand/OneDrive/Desktop/v1/airbnb_train_y_price_gte_150.csv\")\n",
    "y_test = pd.read_csv(\"C:/Users/ssand/OneDrive/Desktop/v1/airbnb_test_y_price_gte_150.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0675cdf1-8b13-4d97-a8c4-8d440b859687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ssand\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:306: UserWarning: The total space of parameters 16 is smaller than n_iter=500. Running 16 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ssand\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best precision score is 0.9370404920282969\n",
      "... with parameters: {'kernel': 'poly', 'gamma': 0.01, 'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "#SVM classification using Random search\n",
    "\n",
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "     'C': [0.1,1, 10, 100], \n",
    "    'gamma': [1,0.1,0.01,0.001],\n",
    "    'kernel': ['poly']\n",
    "}\n",
    "\n",
    "RN_search = SVC()\n",
    "rand_search = RandomizedSearchCV(estimator = RN_search, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestPrecisionTree = rand_search.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae8814-8e90-40bb-82a3-e4a7f4954b1e",
   "metadata": {},
   "source": [
    "The best precision score is 0.9370404920282969"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "688c08b9-4072-44bf-8aee-3fc1c1d373b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ssand\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perfect  precision score as far as  0.9370404920282969\n",
      "... with parameters: {'C': 0.1, 'gamma': 0.01, 'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "#SVM classification using Grid search\n",
    "\n",
    "\n",
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "     'C': [0.1,1, 10, 100], \n",
    "     'gamma': [1,0.1,0.01,0.001],\n",
    "    'kernel': ['poly']\n",
    "}\n",
    "\n",
    "G_search = SVC()\n",
    "grid_search = GridSearchCV(estimator = G_search, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The perfect  {score_measure} score as far as  {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestPrecisionTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da716aaa-80ae-4ccb-9d74-00db411a4c0c",
   "metadata": {},
   "source": [
    "The perfect  precision score as far as  0.9370404920282969"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "979ea174-b5fe-426e-96ac-a193abde42a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The  precision score is 0.8570532608300236\n",
      "... with parameters: {'min_samples_split': 21, 'min_samples_leaf': 5, 'min_impurity_decrease': 0.0016, 'max_leaf_nodes': 37, 'max_depth': 25, 'criterion': 'entropy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ssand\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "20 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ssand\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ssand\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\tree\\_classes.py\", line 969, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\ssand\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\tree\\_classes.py\", line 265, in fit\n",
      "    check_scalar(\n",
      "  File \"C:\\Users\\ssand\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\validation.py\", line 1480, in check_scalar\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split == 1, must be >= 2.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ssand\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.82488631 0.82751398 0.77844887 0.82627955 0.83247073 0.82514459\n",
      " 0.82488631 0.82488631 0.82488631 0.82488631 0.83026894 0.82855513\n",
      " 0.82488631 0.82509452 0.82487201 0.82769013 0.83585906 0.82627955\n",
      " 0.82488631 0.82488631 0.84619182 0.82488631 0.82488631 0.82488631\n",
      " 0.83556602 0.82514459 0.82488631 0.82488631 0.82487201 0.84283953\n",
      " 0.83042632 0.85705326 0.82488631 0.82488631 0.82488631 0.82488631\n",
      " 0.82614606 0.82488631 0.8360193  0.82514459 0.82488631 0.84062648\n",
      " 0.82488631 0.82488631 0.83766907 0.82488631 0.85540105 0.82488631\n",
      " 0.83585906 0.84679758 0.82488631 0.83987605 0.82768187 0.82514459\n",
      " 0.8280068  0.83204939 0.82756901 0.82488631 0.83302929 0.82488631\n",
      " 0.82841455 0.83019972 0.82488631 0.82488631 0.82488631 0.85063152\n",
      " 0.84073709 0.85015226 0.82488631 0.83353865 0.84708677 0.82488631\n",
      " 0.8360193  0.82488631 0.85325455 0.77844887 0.82488631 0.82488631\n",
      " 0.82488631 0.83271256 0.82488631 0.84409773 0.82488631 0.83581309\n",
      " 0.83847851 0.83367319 0.83331313 0.82450495 0.83218559 0.82488631\n",
      " 0.83019424 0.84699756 0.77844887 0.82488631 0.82488631 0.82488631\n",
      " 0.83317591 0.82488631 0.82488631 0.82488631 0.82756901 0.82488631\n",
      " 0.83247073 0.83987605 0.8312966  0.82488631 0.82488631 0.82488631\n",
      " 0.82488631 0.82488631 0.82488631 0.82488631 0.82488631 0.84111541\n",
      " 0.82488631 0.82488631 0.82841043 0.82488631 0.82514459 0.83585906\n",
      " 0.8450845  0.82488631 0.84039827 0.82768187 0.82488631 0.82488631\n",
      " 0.82856943 0.83024761 0.82488631 0.82488631 0.84162216 0.82488631\n",
      " 0.82488631 0.82488631 0.82488631 0.82488631 0.82514459 0.82488631\n",
      " 0.82627955 0.82488631 0.82488631 0.82488631 0.82488631 0.83952217\n",
      " 0.82488631 0.83416053 0.84760714 0.82488631 0.84679758 0.77844887\n",
      " 0.83585906 0.82488631 0.82566132 0.82488631 0.82488631 0.82488631\n",
      " 0.82488631 0.77844887 0.82488631 0.82488631 0.83585906 0.83585906\n",
      " 0.82488631 0.8389098  0.77844887 0.82919337 0.83541047 0.82488631\n",
      " 0.82842473 0.82488631 0.77844887 0.82978518 0.82488631 0.82488631\n",
      " 0.83340782 0.8353133  0.82488631 0.82487201 0.82756901 0.82488631\n",
      " 0.82488631 0.82488631 0.83520465 0.8278815  0.83247073 0.82488631\n",
      " 0.83954753 0.77844887 0.82488631 0.83761924 0.82488631 0.84679758\n",
      " 0.82487201 0.82488631 0.82488631 0.82488631 0.82926049 0.83917863\n",
      " 0.82488631 0.82488631 0.82488631 0.82488631 0.82488631 0.83019972\n",
      " 0.84523013 0.83585906 0.83989593 0.82627955 0.84486179 0.82488631\n",
      " 0.8278815  0.82514459 0.82488631 0.8278815  0.82488631 0.82488631\n",
      " 0.82756901 0.84297634 0.8360193  0.82488631 0.82488631 0.77844887\n",
      " 0.82488631 0.82488631 0.82488631 0.84409759 0.83585906 0.82488631\n",
      " 0.83414014 0.84864413 0.83585906 0.84250571 0.82488631 0.82895597\n",
      " 0.82488631 0.82488631 0.82488631 0.82488631 0.82488631 0.82488631\n",
      " 0.82488631 0.82488631 0.83834604 0.83247073 0.82488631 0.82488631\n",
      " 0.82856943 0.82488631 0.82488631 0.83247073 0.83954218 0.82514459\n",
      " 0.84984481 0.8418381  0.82488631 0.83247073 0.82488631 0.8312966\n",
      " 0.82488631 0.82488631 0.82856943 0.84062648 0.83235541 0.82514459\n",
      " 0.84462751 0.77844887 0.82857656 0.82488631 0.82488631 0.82488631\n",
      " 0.83987605 0.82488631 0.82488631 0.77844887 0.82488631 0.82490421\n",
      " 0.82488631 0.83304336 0.83766907 0.82039937 0.82627955 0.82488631\n",
      " 0.83766907 0.84062648 0.82488631 0.83247073 0.82488631 0.84276655\n",
      " 0.82488631 0.82488631 0.82855513 0.82488631 0.82488631 0.82488631\n",
      " 0.83019424 0.77844887 0.82514459 0.82488631 0.83952217 0.83015496\n",
      " 0.82768187 0.77844887 0.82488631 0.8418381  0.82488631 0.8325115\n",
      " 0.82488631 0.77844887 0.8331412  0.82488631 0.82488631 0.82488631\n",
      " 0.83128765 0.82488631 0.82578511 0.83077653 0.8312966  0.82488631\n",
      " 0.82488631 0.83585906 0.82488631 0.82488631 0.82926049 0.82488631\n",
      "        nan 0.83581309 0.82507291 0.82979298 0.82488631 0.84613799\n",
      " 0.82488631 0.82488631 0.83212638 0.82488631 0.82487201 0.82488631\n",
      " 0.83766907 0.83687115 0.77844887 0.82488631 0.83204939 0.82488631\n",
      " 0.83019972 0.83812449 0.77844887 0.83133873 0.83585906        nan\n",
      " 0.83137325 0.83581309 0.83781623 0.8292201  0.82514459 0.84727065\n",
      " 0.84375272 0.82829096 0.82514459 0.82488631 0.82488631 0.82488631\n",
      " 0.82488631 0.83581309 0.84250571 0.82488631 0.82488631 0.82488631\n",
      " 0.82488631 0.82488631 0.84143592 0.82488631 0.82856943 0.82488631\n",
      " 0.82514459 0.82488631 0.82488631 0.84189061 0.82488631 0.82488631\n",
      " 0.84740936 0.82868301 0.84624033 0.82514459 0.82487201 0.84287178\n",
      " 0.82488631 0.83944018 0.82488631 0.82488631 0.82488631 0.82712333\n",
      " 0.82756901 0.82488631 0.84462751 0.82488631 0.83585906 0.82488631\n",
      " 0.83863819 0.82488631 0.77844887 0.82488631 0.83585906 0.82488631\n",
      " 0.82869408 0.83585906 0.82488631 0.82488631 0.8312966  0.82488631\n",
      " 0.82488631 0.82488631 0.82488631 0.82488631 0.82488631 0.84805907\n",
      " 0.83585906        nan 0.83024761 0.82488631 0.83585906 0.83556602\n",
      " 0.83898196 0.82488631 0.77844887 0.82488631 0.82514459 0.83581309\n",
      " 0.83585906 0.84592556 0.83500369 0.8317569  0.83585906 0.82488631\n",
      " 0.82488631 0.82488631 0.82490421 0.82488631 0.82488631 0.82488631\n",
      " 0.82487201 0.83766907 0.82488631 0.82488631 0.82488631 0.83877009\n",
      " 0.83585906 0.82488631 0.82488631 0.84679758 0.82901807 0.8280068\n",
      " 0.82488631 0.83304336 0.82488631 0.83766907 0.82488631 0.84779046\n",
      " 0.82488631 0.82514459 0.84740936 0.82488631 0.83956    0.8312966\n",
      " 0.82488631 0.82488631 0.83847851 0.8280068         nan 0.84330633\n",
      " 0.82937615 0.83585906 0.82488631 0.83302929 0.82488631 0.82488631\n",
      " 0.84075524 0.83433522 0.77844887 0.82488631 0.82488631 0.83581309\n",
      " 0.82488631 0.77844887 0.84992865 0.82488631 0.8421527  0.82488631\n",
      " 0.8314787  0.85421748 0.83019972 0.83766907 0.82488631 0.83987605\n",
      " 0.84523013 0.82514459 0.82488631 0.83079154 0.77844887 0.82488631\n",
      " 0.82756901 0.83332056]\n",
      "  warnings.warn(\n",
      "C:\\Users\\ssand\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the train scores are non-finite: [0.82583601 0.86711507 0.77774902 0.83082791 0.83861476 0.82851932\n",
      " 0.82583601 0.82583601 0.82583601 0.82583601 0.8568388  0.83202072\n",
      " 0.82583601 0.84688781 0.82811686 0.83061584 0.84341642 0.83082791\n",
      " 0.82583601 0.82583601 0.86622739 0.82583601 0.82583601 0.82583601\n",
      " 0.8428255  0.82851932 0.82583601 0.82583601 0.82814917 0.8644927\n",
      " 0.85112061 0.88519014 0.82583601 0.82583601 0.82583601 0.82583601\n",
      " 0.82855599 0.82583601 0.84408099 0.82851932 0.82583601 0.84469235\n",
      " 0.82583601 0.82583601 0.84452029 0.82583601 0.87701059 0.82583601\n",
      " 0.84341642 0.8548865  0.82583601 0.84373893 0.83952466 0.82851932\n",
      " 0.83272552 0.83690543 0.83093316 0.82583601 0.83941205 0.82583601\n",
      " 0.83487784 0.83578976 0.82583601 0.82583601 0.82583601 0.86524418\n",
      " 0.85935688 0.86402075 0.82583601 0.83948833 0.85516646 0.82583601\n",
      " 0.84408099 0.82583601 0.86737887 0.77774902 0.82583601 0.82583601\n",
      " 0.82583601 0.83779112 0.82583601 0.8558059  0.82583601 0.84238417\n",
      " 0.86403712 0.84029205 0.85765731 0.883894   0.8379911  0.82583601\n",
      " 0.83572419 0.88495857 0.77774902 0.82583601 0.82583601 0.82583601\n",
      " 0.83889216 0.82583601 0.82583601 0.82583601 0.83093316 0.82583601\n",
      " 0.83861476 0.84373893 0.83504222 0.82583601 0.82583601 0.82583601\n",
      " 0.82583601 0.82583601 0.82583601 0.82583601 0.82583601 0.86207899\n",
      " 0.82583601 0.82583601 0.83235552 0.82583601 0.82851932 0.84341642\n",
      " 0.85286541 0.82583601 0.87189819 0.83952466 0.82583601 0.82583601\n",
      " 0.82957533 0.83524992 0.82583601 0.82583601 0.87753533 0.82583601\n",
      " 0.82583601 0.82583601 0.82583601 0.82583601 0.82851932 0.82583601\n",
      " 0.83082791 0.82583601 0.82583601 0.82583601 0.82583601 0.84630491\n",
      " 0.82583601 0.85715966 0.85763263 0.82583601 0.8548865  0.77774902\n",
      " 0.84341642 0.82583601 0.82855289 0.82583601 0.82583601 0.82583601\n",
      " 0.82583601 0.77774902 0.82583601 0.82583601 0.84341642 0.84341642\n",
      " 0.82583601 0.84840174 0.77774902 0.85642549 0.84181549 0.82583601\n",
      " 0.83004236 0.82583601 0.77774902 0.85375413 0.82583601 0.82583601\n",
      " 0.86471369 0.87301512 0.82583601 0.82814917 0.83093316 0.82583601\n",
      " 0.82583601 0.82583601 0.86759934 0.83301112 0.83861476 0.82583601\n",
      " 0.85223784 0.77774902 0.82583601 0.87676269 0.82583601 0.8548865\n",
      " 0.82811686 0.82583601 0.82583601 0.82583601 0.83452251 0.8633291\n",
      " 0.82583601 0.82583601 0.82583601 0.82583601 0.82583601 0.83578976\n",
      " 0.85216545 0.84341642 0.84941419 0.83082791 0.8745043  0.82583601\n",
      " 0.83301112 0.82851932 0.82583601 0.83301112 0.82583601 0.82583601\n",
      " 0.83093316 0.87867355 0.84408099 0.82583601 0.82583601 0.77774902\n",
      " 0.82583601 0.82583601 0.82583601 0.851488   0.84341642 0.82583601\n",
      " 0.86639261 0.87284145 0.84341642 0.84772977 0.82583601 0.8543799\n",
      " 0.82583601 0.82583601 0.82583601 0.82583601 0.82583601 0.82583601\n",
      " 0.82583601 0.82583601 0.84634707 0.83861476 0.82583601 0.82583601\n",
      " 0.82973987 0.82583601 0.82583601 0.83861476 0.84715573 0.82851932\n",
      " 0.87013979 0.84853317 0.82583601 0.83861476 0.82583601 0.83504222\n",
      " 0.82583601 0.82583601 0.82973987 0.84469235 0.84091496 0.82851932\n",
      " 0.84981229 0.77774902 0.84022157 0.82583601 0.82583601 0.82583601\n",
      " 0.84373893 0.82583601 0.82583601 0.77774902 0.82583601 0.84761819\n",
      " 0.82583601 0.84049152 0.84452029 0.83620896 0.83082791 0.82583601\n",
      " 0.84452029 0.84469235 0.82583601 0.83861476 0.82583601 0.84810619\n",
      " 0.82583601 0.82583601 0.83202072 0.82583601 0.82583601 0.82583601\n",
      " 0.83572419 0.77774902 0.82848866 0.82583601 0.84630491 0.83676532\n",
      " 0.83955617 0.77774902 0.82583601 0.84834074 0.82583601 0.83800353\n",
      " 0.82583601 0.77774902 0.8410571  0.82583601 0.82583601 0.82583601\n",
      " 0.83482563 0.82583601 0.82845794 0.83461817 0.83504222 0.82583601\n",
      " 0.82583601 0.84341642 0.82583601 0.82583601 0.83397202 0.82583601\n",
      "        nan 0.84238417 0.83803457 0.83862749 0.82583601 0.85882553\n",
      " 0.82583601 0.82583601 0.84917181 0.82583601 0.82814917 0.82583601\n",
      " 0.84452029 0.85807156 0.77774902 0.82583601 0.8369362  0.82583601\n",
      " 0.83578976 0.89837095 0.77774902 0.83647977 0.84341642        nan\n",
      " 0.83435515 0.84238417 0.86166046 0.86191124 0.82851932 0.86209664\n",
      " 0.85732642 0.84935738 0.82851932 0.82583601 0.82583601 0.82583601\n",
      " 0.82583601 0.84238417 0.84770077 0.82583601 0.82583601 0.82583601\n",
      " 0.82583601 0.82583601 0.85178595 0.82583601 0.82957533 0.82583601\n",
      " 0.82851932 0.82583601 0.82583601 0.84732778 0.82583601 0.82583601\n",
      " 0.85400728 0.83269501 0.85973419 0.82848866 0.82814917 0.85547508\n",
      " 0.82583601 0.86319229 0.82583601 0.82583601 0.82583601 0.8486979\n",
      " 0.83093316 0.82583601 0.84981229 0.82583601 0.84341642 0.82583601\n",
      " 0.85634039 0.82583601 0.77774902 0.82583601 0.84341642 0.82583601\n",
      " 0.83460667 0.84341642 0.82583601 0.82583601 0.83504222 0.82583601\n",
      " 0.82583601 0.82583601 0.82583601 0.82583601 0.82583601 0.85843965\n",
      " 0.84341642        nan 0.83524992 0.82583601 0.84341642 0.8428255\n",
      " 0.87613942 0.82583601 0.77774902 0.82583601 0.82851932 0.84238417\n",
      " 0.84341642 0.87302962 0.84257766 0.83864725 0.84341642 0.82583601\n",
      " 0.82583601 0.82583601 0.84761819 0.82583601 0.82583601 0.82583601\n",
      " 0.82814917 0.84452029 0.82583601 0.82583601 0.82583601 0.86096842\n",
      " 0.84341642 0.82583601 0.82583601 0.8548865  0.8488114  0.83272552\n",
      " 0.82583601 0.84049152 0.82583601 0.84452029 0.82583601 0.86546226\n",
      " 0.82583601 0.82851932 0.85400728 0.82583601 0.85571576 0.83504222\n",
      " 0.82583601 0.82583601 0.86403712 0.83272552        nan 0.85883355\n",
      " 0.83455062 0.84341642 0.82583601 0.83941205 0.82583601 0.82583601\n",
      " 0.84771858 0.83790805 0.77774902 0.82583601 0.82583601 0.84238417\n",
      " 0.82583601 0.77774902 0.86391213 0.82583601 0.85208791 0.82583601\n",
      " 0.85510827 0.87214664 0.83578976 0.84452029 0.82583601 0.84373893\n",
      " 0.85216545 0.82851932 0.82583601 0.85519587 0.77774902 0.82583601\n",
      " 0.83093316 0.83993316]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Decision tree using Random search\n",
    "\n",
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,70),  \n",
    "    'min_samples_leaf': np.arange(1,80),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 100), \n",
    "    'max_depth': np.arange(1,50), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The  {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestPrecisionTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43e8fa7-5879-42cc-bb6a-7d79ceb3c082",
   "metadata": {},
   "source": [
    "The  precision score is 0.8570532608300236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68a7f7fa-d323-4ddb-b091-4a44c1cb9e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4032 candidates, totalling 20160 fits\n",
      "The precision score is 0.8613399498900961\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 23, 'max_leaf_nodes': 35, 'min_impurity_decrease': 0.001, 'min_samples_leaf': 4, 'min_samples_split': 20}\n"
     ]
    }
   ],
   "source": [
    "#Decision tree using Grid search\n",
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(18,24),  \n",
    "    'min_samples_leaf': np.arange(3,7),\n",
    "    'min_impurity_decrease': np.arange(0.0010, 0.0017, 0.0001),\n",
    "    'max_leaf_nodes': np.arange(34,40), \n",
    "    'max_depth': np.arange(23,27), \n",
    "    'criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "Dout_GS = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = Dout_GS, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestPrecisionTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f65e7ec-46bf-40c8-9099-88b3c704bb80",
   "metadata": {},
   "source": [
    "Conclusion from above :\n",
    "    \n",
    "    Clearly, we can see that the Svm Classification is performming better compared to decision tree with the given data set. So SVM is the better performming model here.\n",
    "    \n",
    "    * The SVM with random search has percision value as 0.93 and for SVM with grid search has 0.93.\n",
    "    \n",
    "    * The Decision tree with random search has percision value is 0.85 and for the Decision tree with grid search has percision value as 0.86 if we compare in between both internally the decision tree with random search is performing better.\n",
    "   \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
